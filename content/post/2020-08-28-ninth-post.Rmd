---
title: Covid Mapping with FSAs and Dissemination Areas
author: ''
date: '2020-08-28'
slug: sixth-post
categories:
  - R
tags: []
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE)
```

## Covid Mapping Part 2

The [first](https://kieranshah.netlify.app/2020/08/26/covid-mapping/) post looked at how we can use the center of a Forward Sortation Area (FSA) into a Census Tract. However, as noted, this is a messy process. FSAs are large ares, see the [Wikipedia page](https://en.wikipedia.org/wiki/List_of_postal_codes_of_Canada:_M). Instead of this messy process, we can look at FSAs, and use Dissemination Areas from Census 2016 to fit those into FSAs. A complete guide to this process can be found on the Statistics Canada website, [here](file:///C:/Users/Kieran%20Shah/Downloads/92-179-g2011001-eng.pdf). 

Dissemination Areas (DA) are the smallest unit Statistics Canada reports. They usually range between 400 and 700 [people](https://en.wikipedia.org/wiki/Census_geographic_units_of_Canada#Dissemination_areas).

This post first downloads the FSA [shapefile](https://www12.statcan.gc.ca/census-recensement/alternative_alternatif.cfm?l=eng&dispext=zip&teng=lfsa000b16a_e.zip&k=%20%20%20%2044221&loc=http://www12.statcan.gc.ca/census-recensement/2011/geo/bound-limit/files-fichiers/2016/lfsa000b16a_e.zip) from Statistics Canada. It then uses the `cancensus` package to extract median income, and other demographic data at the DA level. 

```{r pckgs}

### Read in shape file
library(sf)
library(rgdal)
library(dplyr)
library(leaflet)
library(brms)
library(ggplot2)
library(gt)

```

## Getting the Data

The first step is to bring in the Statistics Canada shapefile. The shapefile is projected in the PCS Lambert Conformal Conic CRS. We need to change this to the WGS coordinate system. To do this, we use the `sf` package, and the function `st_transform`. This makes the coordinate system easier to use, and can also be used with the `leaflet` package.  


```{r CreateShape , echo = TRUE, eval = FALSE}


s.sf <- st_read("C:/Users/Kieran Shah/Downloads/FSA/lfsa000b16a_e.shp")
shape_proj<-st_transform(s.sf, CRS("+init=epsg:4326") )

CovidTO = readr::read_csv("C:/Users/Kieran Shah/Downloads/COVID19 cases.csv") %>% 
  count(FSA) %>% 
  filter(!is.na(FSA))

shpe_proj2 = shape_proj %>% 
  filter(stringi::stri_sub(CFSAUID, 1, 1) %in% c( "M") | 
           stringi::stri_sub(CFSAUID, 1, 2) %in% c( "L1", "L3", "L4", "L5", "L6", "L7")) %>% 
  left_join(CovidTO, by = c("CFSAUID" = "FSA")) %>% 
  mutate(n = ifelse(is.na(n), 0, n))
  

```


The next step is to get the Census 2016 data. This is the same process as the [previous post](https://kieranshah.netlify.app/2020/08/26/covid-mapping/).  

```{r GetCensus , echo = TRUE, eval = FALSE}

options(cancensus.api_key = "API Key")
options(cancensus.cache_path = "Path")

census_data <- get_census(dataset='CA16', regions=list(CMA="35535"),
                         vectors=c("v_CA16_2397",
                                   "v_CA16_4002",
                                   "v_CA16_4014",
                                   "v_CA16_4044",
                                   "v_CA16_4266",
                                   "v_CA16_4329",
                                   "v_CA16_4404",
                                   "v_CA16_4608",
                                   "v_CA16_4806",
                                   "v_CA16_425",
                                   "v_CA16_385" ,
                                   "v_CA16_388"	,
                                   "v_CA16_2552"	), level='DA', quiet = TRUE,
                         geo_format = 'sf', labels = 'short')
```


After getting the 2016 Census data, we join the FSA shapefile, and the Census data. Similar to the [previous post](https://kieranshah.netlify.app/2020/08/26/covid-mapping/), we use the `st_join` function. We now have the DA for each of the City of Toronto FSAs.  

To get the Census 2016 variables at the FSA level, we take the weighted mean, using population. FSAs vary in population, which is why we use the weighted mean. 

The mean number of DAs per FSA in the City of Toronto is 59, with a median of 55, and a minimum of 7, and a maximum of a 119. 

For the City of Toronto, there are 96 FSAs. Below is the same `leaflet` plot from the [previous post](https://kieranshah.netlify.app/2020/08/26/covid-mapping/), but using City of Toronto FSAs. 

```{r FSA_Plt, echo=FALSE}

CovidSummaryFSA_wMap = dget( "CovidIncomeData/FSAFinalDS")

CovidMap_MapF = CovidSummaryFSA_wMap %>% 
  rename(MedIncome = v_CA16_2397,
       NA_Aboriginal = v_CA16_4002, 
       Other_NA = v_CA16_4014,
       European = v_CA16_4044,
       Caribbean = v_CA16_4266,
       Latin = v_CA16_4329,
       African = v_CA16_4404,
       Asian = v_CA16_4608, 
       Oceania = v_CA16_4806,
       HHsize = v_CA16_425,
       Less_15 = v_CA16_385 , 
       Btw_14_64 = v_CA16_388	,
       Older64 = v_CA16_2552,
       Cases = n) 

CovidMap_MapF_Tor = CovidMap_MapF %>% 
  mutate(MedIncPct = ntile(MedIncome,10),
         Older64Pct = ntile(Older64,10)) %>% 
  mutate_at(vars(MedIncPct, Older64Pct) , ~paste0(., "0 Percentile")) %>% 
  filter(CD_UID == "3520" & stringi::stri_sub(CFSAUID,1,1) == "M")

cov_popup <- paste0("<strong>Covid Cases: </strong>", 
                        CovidMap_MapF_Tor$Cases , 
                    "<br><strong>Median Income: </strong>", 
                        CovidMap_MapF_Tor$MedIncPct ,
                    "<br><strong>Senior Share: </strong>", 
                        CovidMap_MapF_Tor$Older64Pct)

bins <- c(0, 10,50, 100,200, 300,400, 500, 1000)
pal <- colorBin("RdYlBu", domain = CovidMap_MapF_Tor$Cases, bins = bins)

CovidMapLeaf = leaflet(CovidMap_MapF_Tor ) %>% 
  addProviderTiles(providers$CartoDB.Positron) %>%
  addPolygons(fillColor = ~pal(Cases),
              color = "white",
              weight = 1,
              opacity = 1,
              fillOpacity = 0.65,
              popup = cov_popup) %>% 
  addLegend("bottomright", pal = pal, values = ~CovidMap_MapF_Tor$Cases,
    title = "Covid Cases",
    opacity = 1)

```

As we can see, the greater case loads are on the outer FSA of the City of Toronto. 

## Modelling

In the previous post, we saw a right-skewed distribution. The data is again skewed to the right. Instead of using a negative binomial model, or zero-inflated model, we will try `skew normal` family from the `brms` family. 

We first compare the `skew_normal` model to a Gaussian model. Again, we use the `get_prior` function to get the skewness prior, and the other priors we need. When we compare Gaussian model to the skew model, the skew model is clearly superior to the Gaussian model using Widely Applicable Information Criterion (WAIC). 

We also try increasing the alpha above zero to account for greater skewness. This model is in fact worse, and using the prior on alpha of 0 mean, and 4 standard deviation as the `get_prior` function recommends. 

We next follow the same process as before, trying different parameters, and comparing the WAIC. 

```{r densPlt}

CovidMap_MapF %>% 
  filter(CD_UID == "3520" & stringi::stri_sub(CFSAUID, 1,1) == "M") %>% 
  ggplot(., aes(x = Cases)) +
  geom_density() +
  theme_classic()

```


```{r mdling, results = 'hide'}


DataMdl = CovidMap_MapF %>% 
 # select(-PRUID, -PRNAME, -geometry) %>% 
#  as_tibble() %>% 
  mutate_at(vars(MedIncome:Older64) , ~scale(.)[,1] ) %>% 
  mutate(Split = case_when(CD_UID == "3520" & stringi::stri_sub(CFSAUID,1,1) == "M" ~ "Toronto",
                           TRUE ~ "GTA")) %>% 
  mutate(MedIncome_2 = MedIncome**2)
  
Gaussian_1 = brm(data = DataMdl %>% filter(Split == "Toronto"), family = gaussian(),
    Cases ~ MedIncome + Asian + HHsize + Older64,
    prior = c(prior(student_t(3, 107, 100.1), class = Intercept),
              prior(normal(0, 2), class = b),
              prior(student_t(3, 0, 100.1), class = sigma)),
      iter = 3000, warmup = 1000, cores = 2, chains = 4,
      seed = 11) 


SkewMdl_2 <- brm(data = DataMdl %>% filter(Split == "Toronto"), family = skew_normal(),
    Cases ~ MedIncome + Asian + Caribbean + Latin + NA_Aboriginal + African + 
                         HHsize + Older64,
              prior = c(prior(student_t(3, 107, 100.1), class = Intercept),
              prior(normal(0, 2), class = b),
              prior(student_t(3, 0, 100.1), class = sigma),
              prior(normal(0,4) , class = alpha)),
      iter = 3000, warmup = 1000, cores = 2, chains = 4,
      seed = 11)
```


```{r skewnorm2 , eval = FALSE}


SkewNorm_2 = brm(data = DataMdl %>% filter(Split == "Toronto"), family = skew_normal(),
    Cases ~ MedIncome + Asian + HHsize + Older64,
    prior = c(prior(student_t(3, 107, 100.1), class = Intercept),
              prior(normal(0, 2), class = b),
              prior(student_t(3, 0, 100.1), class = sigma),
              prior(normal(0,4) , class = alpha)),
      iter = 3000, warmup = 1000, cores = 2, chains = 4,
      seed = 11)

Gaussian_1 <- add_criterion(Gaussian_1, "waic")
SkewNorm <- add_criterion(SkewNorm, "waic")
SkewNorm_2 <- add_criterion(SkewNorm_2, "waic")

mdlWeightsZeroInfl = brms::model_weights(SkewNorm_2, SkewNorm,
                                 weights = "waic")

```


```{r PostPlts}

# posterior predictive checking
GaussianPP = pp_check(Gaussian_1, nsamples = 1e2)
SkewPP = pp_check(SkewMdl_2, nsamples = 1e2) 

```

The plot below provide the posterior checks for the Gaussian model. We use the function `pp_check`. A good vignette can be found [here](https://cran.r-project.org/web/packages/bayesplot/vignettes/graphical-ppcs.html). As we can see the Gaussian model does not fit the data well.  


```{r GaussianPP_plt}

GaussianPP

```

Next, we show the same posterior checks for the skewed model:

```{r skew_plt}

SkewPP

```

As we can see the data fit the model much better. 

We next follow the same process as before, trying different parameters, and comparing the WAIC.

```{r mdlComp , eval = FALSE}

SkewMdl_1 <- brm(data = DataMdl %>% filter(Split == "Toronto"), family = skew_normal(),
    Cases ~ MedIncome + Asian + HHsize + Older64,
              prior = c(prior(student_t(3, 107, 100.1), class = Intercept),
              prior(normal(0, 2), class = b),
              prior(student_t(3, 0, 100.1), class = sigma),
              prior(normal(0,4) , class = alpha)),
      iter = 3000, warmup = 1000, cores = 2, chains = 4,
      seed = 11)


SkewMdl_2 <- brm(data = DataMdl %>% filter(Split == "Toronto"), family = skew_normal(),
    Cases ~ MedIncome + Asian + Caribbean + Latin + NA_Aboriginal + African + 
                         HHsize + Older64,
              prior = c(prior(student_t(3, 107, 100.1), class = Intercept),
              prior(normal(0, 2), class = b),
              prior(student_t(3, 0, 100.1), class = sigma),
              prior(normal(0,4) , class = alpha)),
      iter = 3000, warmup = 1000, cores = 2, chains = 4,
      seed = 11)

SkewMdl_3 <- brm(data = DataMdl %>% filter(Split == "Toronto"), family = skew_normal(),
    Cases ~ MedIncome + Asian + Caribbean + Latin + NA_Aboriginal + African + 
                         HHsize + Older64 + MedIncome_2,
              prior = c(prior(student_t(3, 107, 100.1), class = Intercept),
              prior(normal(0, 2), class = b),
              prior(student_t(3, 0, 100.1), class = sigma),
              prior(normal(0,4) , class = alpha)),
      iter = 3000, warmup = 1000, cores = 2, chains = 4,
      seed = 11)  

SkewMdl_4 <- brm(data = DataMdl %>% filter(Split == "Toronto"), family = skew_normal(),
    Cases ~ MedIncome * Older64 + Asian + Caribbean + Latin + NA_Aboriginal + African + 
                         HHsize,
              prior = c(prior(student_t(3, 107, 100.1), class = Intercept),
              prior(normal(0, 2), class = b),
              prior(student_t(3, 0, 100.1), class = sigma),
              prior(normal(0,4) , class = alpha)),
      iter = 3000, warmup = 1000, cores = 2, chains = 4,
      seed = 11)  
  

SkewMdl_5 <- brm(data = DataMdl %>% filter(Split == "Toronto"), family = skew_normal(),
    Cases ~ MedIncome * HHsize  + Asian + Caribbean + Latin + NA_Aboriginal + African + 
                         Older64,
              prior = c(prior(student_t(3, 107, 100.1), class = Intercept),
              prior(normal(0, 2), class = b),
              prior(student_t(3, 0, 100.1), class = sigma),
              prior(normal(0,4) , class = alpha)),
      iter = 3000, warmup = 1000, cores = 2, chains = 4,
      seed = 11)  
  
SkewMdl_1 <- add_criterion(SkewMdl_1, "waic")
SkewMdl_2 <- add_criterion(SkewMdl_2, "waic")
SkewMdl_3 <- add_criterion(SkewMdl_3, "waic")
SkewMdl_4 <- add_criterion(SkewMdl_4, "waic")
SkewMdl_5 <- add_criterion(SkewMdl_5, "waic")
  
mdlWeights = brms::model_weights(SkewMdl_1,SkewMdl_2, SkewMdl_3,
                                         SkewMdl_4, SkewMdl_5,
                                 weights = "waic")



```

There is not a considerable difference between any of the models from two to five. There is no difference, and thus, we use the most parsimonious model, which is the second model. 

```{r MdlWeightsTbl}

mdlWeights = dget("CovidIncomeData/MdlWeights")

tibble(`Model Names` = names(mdlWeights),
       Weight = mdlWeights) %>% 
  gt()

```


The final model includes:  

1. Median Income
2. Share of different ethnicities as a percent of total population
3. Household size
4. Share of population older the age of 64

Below, we see the posterior estimates for the four parameters: 

```{r mdlResults}

PredictCovidOther = DataMdl %>% 
  filter(Split != "Toronto")

Predicted = predict(SkewMdl_2 , PredictCovidOther ) %>% as_tibble() %>% select(Estimate)

PredictedValues = bind_cols(PredictCovidOther, Predicted)   


Posteriors= brms::posterior_samples(SkewMdl_2)

MakeCls = Posteriors %>% 
  select(b_MedIncome:b_Older64) %>% 
  tidyr::gather(Nm, Value) %>%
  group_by(Nm) %>% 
  summarise(p90 = quantile(Value, .90),
            p10 = quantile(Value, .10),
            p50 = quantile(Value, .5)) %>% 
  ungroup()


PostCoefPlt = Posteriors %>% 
  select(b_MedIncome:b_Older64) %>% 
  tidyr::gather(Nm, Value) %>% 
  inner_join(MakeCls, by = "Nm") %>% 
  mutate(Clr = case_when(Value < p90 &  Value > p10 ~ "Bulk", 
                         TRUE ~ "Tails")) %>% 
  ggplot(., aes(x = Value, fill= Clr)) + 
  geom_histogram() +
  theme_classic() +
  theme(legend.position = "none") +
  geom_vline(xintercept = 0) +
  facet_wrap(~Nm, scales = "free", ncol = 2)
  

```

The plot below shows the distribution of the posterior for the four parameters. The red shaded area is between the 10th and 90th percentile of the posterior distribution. The blue is the tails of the posterior distribution. As we can see, the bulk of almost all the parameters' posterior centers around zero, and no effect. However, again, we see that the median income parameter does have a considerable amount of its distribution below zero. 

```{r postcoefdistr}

PostCoefPlt
```

Finally, we again look at the predicted cases by Census District. Similar to the [previous post](https://kieranshah.netlify.app/2020/08/26/covid-mapping/), we look at [Peel Region](https://www.peelregion.ca/coronavirus/case-status/), there are 7513 cases as of August 25. 

To extrapolate the Peel Covid-19 cases for August 19, we assume a constant rate of growth of 31 new cases over 15 days, which is `r 31*15`. The skew model overestimates the actual positive Covid-19 cases by a considerable amount.  

```{r PredictedByCD}

TblEst = PredictedValues %>%
  filter(Split == "GTA") %>% 
  mutate(CD_UID = case_when(stringi::stri_sub(CFSAUID, 1, 2) %in% c("L4", "L5") ~ "3521",
                            TRUE ~ CD_UID)) %>%
  as_tibble() %>% 
  select(CD_UID, Estimate) %>% 
  mutate(Estimate = round(Estimate, digits = 0)) %>% 
  group_by(CD_UID) %>% 
  summarise(`Skew Normal Model` = sum(Estimate)) %>% 
  ungroup() %>%
  bind_rows(
    summarise_if(., is.numeric, sum) %>% 
      mutate(CD_UID = "Total")
  ) 

PredictedVals_GTA =  PredictedValues %>%
  filter(Split == "GTA")

TblEst %>% 
  gt()

```

Again, we map the expected Covid-19 cases for Census Districts that are not the City of Toronto. The range is very restricted. The minimum value is `r min(PredictedVals_GTA$Estimate)`, and the max value is `r max(PredictedVals_GTA$Estimate)`. This is simply because the model is not very good. All of the estimates are primarily based on the intercept.  

```{r exp_map}

Predcov_popup <- paste0("<br><strong>Covid Cases: </strong>", 
                        round(PredictedVals_GTA$Estimate))

bins <- c(0, 150,155, 160,165, 170,175, 200)
pal <- colorBin("RdYlBu", domain = PredictedVals_GTA$Estimate, bins = bins)

CovidPredictedLeaf = leaflet(PredictedVals_GTA) %>% 
  addProviderTiles(providers$CartoDB.Positron) %>%
  addPolygons(fillColor = ~pal(Estimate),
              color = "white",
              weight = 1,
              opacity = 1,
              fillOpacity = 0.65,
              popup = Predcov_popup) %>% 
  addLegend("bottomright", pal = pal, values = ~PredictedVals_GTA$Estimate,
    title = "Predicted Covid Cases",
    opacity = 1)

CovidPredictedLeaf

```



## Limitations

The analysis and Covid-19 testing limitations exist as discussed in the previous post. However, an added challenge is the polygon merge between the Covid-19 FSA shape file and the Census 2016 DA file is imperfect. There are a few FSAs which start with 'L', but are considered to be part of the Census District 3520, which is the City of Toronto. 