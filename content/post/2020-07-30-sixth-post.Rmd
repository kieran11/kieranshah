---
title: New Cases Analysis
author: ''
date: '2020-07-31'
slug: sixth-post
categories:
  - R
tags: []
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE)
```


## Analysis:

The purpose of this analysis is to evaluate the impact of our three weather metrics on **new** Covid-19 cases in California. The complete and large dataset can be found [here](https://www.dropbox.com/preview/Public/WeatherData/WeatherCases.csv?role=personal).

The complete code for this post can be found [here:](https://github.com/kieran11/WeatherCovid/blob/master/CovidAnalysis.Rmd). 


```{r analysis}

library(dplyr)
library(brms)
library(bayesplot)
library(ggplot2)
library(tidyr)
library(rlang)
library(gt)

lags <- function(var, n=10){
  var <- enquo(var)
  
  indices <- seq_len(n)
  purrr::map( indices, ~quo(lag(!!var, !!.x)) ) %>% 
    purrr::set_names(sprintf("lag_%s_%02d", quo_text(var), indices))
  
}


ExtraVariables = dget("WeatherData/ExtraVariables")
CasesWeather = dget( "WeatherData/AnalysisDataSet")

Analysis = CasesWeather %>% 
  group_by( fips) %>% 
  mutate(PrevDayCases = lag(cases),
         PrevDayCases =ifelse(is.na(PrevDayCases), 0, PrevDayCases ) ,
         Days = row_number() ) %>% 
  ungroup() %>% 
  mutate(NewCases = cases - PrevDayCases) %>% 
  group_by(fips) %>% 
  mutate(Lag_NewCases = lag(NewCases),
         !!!lags(Stages, 14)) %>% 
  ungroup() %>% 
  inner_join(ExtraVariables, by = c("fips" = "GEOID")) %>% 
  na.omit(.) %>% 
  mutate_at(vars(starts_with("lag_p"), starts_with("lag_t"), Lag_NewCases,
                 MedianAge, MedianIncome) , ~scale(.x)[,1]) %>% 
  mutate(lag_temp_sqr = lag_temperatureMax_06**2) 
  
  

```

The basic California model will hypothesize that the stages of each opening should increase new cases after a week. Similarly, weather should work in a similar pattern. Better weather should encourage more outdoor activity, and we should see a drop in cases up until a certain point. Temperature rises beyond a certain point will have a positive association with new cases as it encourages more indoor activity. 

The first step is to determine the model. The main choices of the model are the lag number. Does the temperature three days prior have a greater influence on new Covid-19 cases than the temperature 13 days prior? Ideally, we would use a log likelihood model. However, the models are not nested, and thus we do not use a log likelihood test. A nested models contains the previous model within it. 


```{r covidmdl}


FindModelLags = function(x) {
  
  MdlNoInt = as.formula(paste("NewCases ~ Lag_NewCases ", paste0("lag_temperatureMax_", x) , 
                   paste0("lag_Stages_",x), "(1|fips)", "(1|Days)",sep = "+") )
  
  MdlInt = as.formula(paste("NewCases ~ Lag_NewCases ", 
                            paste0("lag_temperatureMax_", x, "*", 
                                   paste0("lag_precipIntensity_", x)) , 
                   paste0("lag_Stages_",x), "(1|fips)", "(1|Days)",sep = "+") )
  
  
ModelCovid = lme4::lmer(MdlNoInt, 
                        data = Analysis)

ModelCovidInt = lme4::lmer(MdlInt, 
                        data = Analysis)
  
FinalMdl = tibble( 
  AICNoInt = broom::glance(ModelCovid) %>% select(AIC) %>% pull(AIC),
  AICInt = broom::glance(ModelCovidInt) %>% select(AIC) %>% pull(AIC),
  Lags = x)


return(FinalMdl)
}

LagModels = purrr::map(as.list(sprintf("%02d", seq(1,14,1))), FindModelLags) %>% 
  bind_rows(.) %>% 
  tidyr::gather(Mdl, Val, AICNoInt, AICInt) %>% 
  arrange(Val)

```

As we can see, the model with a six day lag has the lowest AIC, and it will be the model we evaluate. The `Mdl` column just tells us whether it is an interaction model or not. The interaction model has an interaction between temperature and precipiation intensity.  

```{r BestMdl}

head(LagModels) %>% 
  gt::gt()

```

After deciding on the number of lags, we try modeling an interaction between temperature and precipitation intensity. We also try a second order polynomial as previously described. 

```{r FinalFreqMdl}


FnlMdlNoInt = lme4::lmer(as.formula(paste("NewCases ~ Lag_NewCases ", paste0("lag_temperatureMax_", "06") , 
                   paste0("lag_Stages_","06"), "(1|fips)", "(1|Days)",sep = "+") ), 
                        data = Analysis)
  
FnlMdlInt = lme4::lmer(as.formula(paste("NewCases ~ Lag_NewCases ", 
                            paste0("lag_temperatureMax_", "06", "*", 
                                   paste0("lag_precipIntensity_", "06")) , 
                   paste0("lag_Stages_","06"), "(1|fips)", "(1|Days)",sep = "+") ) ,
                   data = Analysis)

FnlMdlIntSqr = lme4::lmer(as.formula(paste("NewCases ~ Lag_NewCases ", 
                            paste0("lag_temperatureMax_", "06", "*", 
                                   paste0("lag_precipIntensity_", "06")) , 
                            "lag_temp_sqr",
                   paste0("lag_Stages_","06"), "(1|fips)", "(1|Days)",sep = "+") ) ,
                   data = Analysis)


LRTest = lmtest::lrtest(FnlMdlNoInt, FnlMdlInt)$`Pr(>Chisq)` %>% 
  as_tibble() %>% 
  filter(!is.na(value) )

LRTestSqr = lmtest::lrtest(FnlMdlIntSqr, FnlMdlNoInt)$`Pr(>Chisq)` %>% 
  as_tibble() %>% 
  filter(!is.na(value) )

CheckResiduals = broom::augment(FnlMdlIntSqr) %>% 
  ggplot(., aes(x = .fitted, y = .resid)) +
  geom_point() +
  theme_classic()

Coefs = broom::tidy(FnlMdlIntSqr) %>% 
  filter(group == "fixed")

```

The best model is the model with an interaction, and the second order polynomial. We use a likelihood ratio test, as all three models are nested. 


After deciding on the model, we use a Bayesian work flow. Unfortunately, there is not enough computing power to check all of the lags, which is why we previously used a Frequentist work flow.  

The next step is model a prior. Ideally, one can use institutional knowledge to determine the prior. For this modeling exercise, we will use the function `get_prior` from the `brms` package to determine the prior. As a comparison, we also use a weak prior. All of the parameters have been transformed to have a mean of zero, and a standard deviation of one. 

```{r BayesianMdl , eval = FALSE}

CovidBayesianStd <-
  brm(data = Analysis, family = gaussian,
      NewCases ~ Lag_NewCases + lag_temperatureMax_06 * lag_precipIntensity_06 + lag_temp_sqr  + 
        lag_Stages_06 +
        (1 | fips) + (1|Days),
      prior = c(prior(student_t(3, 3, 4.4), class = Intercept),
                prior(student_t(3, 0, 4.4) , class = sd) ),
      iter = 10000, warmup = 4000, chains = 4, cores = 4,
      seed = 12)

CovidBayesian10 <-
  brm(data = Analysis, family = gaussian,
      NewCases ~ Lag_NewCases + lag_temperatureMax_06 * lag_precipIntensity_06 + lag_temp_sqr  + 
        lag_Stages_06 +
        (1 | fips) + (1|Days),
      prior = c(prior(normal(0, 10), class = Intercept),
                prior(cauchy(0, 1), class = sd)),
      iter = 10000, warmup = 4000, chains = 4, cores = 4,
      seed = 12)

CovidBayesian10Census <-
  brm(data = Analysis, family = gaussian,
      NewCases ~ Lag_NewCases + lag_temperatureMax_06 * lag_precipIntensity_06 + lag_temp_sqr  + 
        lag_Stages_06 + MedianAge + MedianIncome +
        (1 | fips) + (1|Days),
      prior = c(prior(normal(0, 10), class = Intercept),
                prior(cauchy(0, 1), class = sd)),
      iter = 10000, warmup = 4000, chains = 4, cores = 4,
      control = list(adapt_delta = 0.9),
      seed = 12)

CovidBayesianStdCensus <-
  brm(data = Analysis, family = gaussian,
      NewCases ~ Lag_NewCases + lag_temperatureMax_06 * lag_precipIntensity_06 + lag_temp_sqr  + 
        lag_Stages_06 + MedianAge + MedianIncome +
        (1 | fips) + (1|Days),
      prior = c(prior(student_t(3, 3, 4.4), class = Intercept),
                prior(student_t(3, 0, 4.4) , class = sd)),
      iter = 10000, warmup = 4000, chains = 4, cores = 4,
      control = list(adapt_delta = 0.9),
      seed = 12)



CovidBayesianStd <- add_criterion(CovidBayesianStd, "waic")
CovidBayesian10 <- add_criterion(CovidBayesian10, "waic")

CovidBayesianStdCensus <- add_criterion(CovidBayesianStdCensus, "waic")
CovidBayesian10Census <- add_criterion(CovidBayesian10Census, "waic")


mdlWeights = brms::model_weights(CovidBayesianStd,CovidBayesian10, CovidBayesianStdCensus,
                                 CovidBayesian10Census , weights = "waic")

LOOComp_a = loo_compare(CovidBayesianStd,CovidBayesian10, CovidBayesianStdCensus,
                                CovidBayesian10Census , criterion = "waic") 

EvaluateDf = LOOComp_a %>% 
  as_tibble() %>% 
  mutate(Models = rownames(LOOComp_a) ) %>% 
  select(Models, elpd_diff, se_diff) %>% 
  inner_join(tibble( Models = names(mdlWeights),
                     ModelWeights = mdlWeights), 
             by = "Models")

```


We use the `brms` function `model_weights`. The `model_weights` provides a weighting system which judges the predictive accuracy of the posterior distribution. For a more extensive discussion, see [here](https://discourse.mc-stan.org/t/model-stacking-and-loo-brms-models/4611). 

The model weights are based on the Widely Applicable Information Criterion (WAIC). For a very good explanation and detailed breakdown, see: [here](https://bookdown.org/ajkurz/Statistical_Rethinking_recoded/overfitting-regularization-and-information-criteria.html#using-information-criteria). 

The below table shows the results. 

```{r looCmp}

dget( "WeatherData/ModelEvaluate") %>% 
  mutate_if(is.numeric, ~round(., digits = 3) ) %>% 
  gt() %>% 
   tab_header(
    title = "Model Evaluation"
    )


```

As we can see there is not a considerable difference between the models. The `elpd` difference is smaller, or a similar size to the standard error. The model weights show that the `CovidBayesianStdCensus` model is the best model, but there is not a considerable difference between any of the four models. There is little difference between a weak, normally distributed prior, and Student's T prior. There is also little difference between the addition of the census variables, and the model that does not have the census variables. 

## Checking the models: 

Before looking at the credible intervals, and the mean estimate from the posterior distribution of our two temperature variables, we need to check the chains.

The chains for all of the models can be found [here](https://github.com/kieran11/WeatherCovid/tree/master/CovidAnalysis_files/figure-gfm). There are four trace plots. 

As we can see the chains follow the caterpillar like shapes. To do these plots, we follow the great Soloman Kurtz [book](https://bookdown.org/ajkurz/Statistical_Rethinking_recoded/markov-chain-monte-carlo.html#markov-chain-monte-carlo-1), which is a `tidyverse` version of [Statistical Rethinking](https://xcelab.net/rm/statistical-rethinking/) by Richard McElreath. 

```{r traces , echo= TRUE}

library(bayesplot)


  TraceOut = mcmc_trace(dget("WeatherData/PosteriorDistr") %>% 
                          filter(Model == "CovidBayesianStdCensus") %>% select(-Model) ,
             facet_args = list(ncol = 3), 
             size = .15) +
    labs(title = paste0("Trace Plots for Model: CovidBayesianStdCensus" )) +
    theme_classic() +
    theme(legend.position = c(.95, .2))

```

```{r tracePlt}

TraceOut

```

### Analysis: 

Next, we look at the posterior distribution of our parameters of interest, the weather variables. When we look at the posterior distribution for the weather variables, we see that the 95% credible intervals include zero for `lag_temperatureMax_06` and `lag_temp_sqr`. The other two weather variables are the interaction between temperature and precipitation intensity, and the lag of precipitation intensity`lag_precipIntensity_06`. The credible intervals for both of these variables exclude zero. 

All of the models can be found [here](https://github.com/kieran11/WeatherCovid/tree/master/CovidAnalysis_files/figure-gfm). 


```{r MainParams}


CheckMainParams = dget("WeatherData/PosteriorDistr") %>%
  filter(Model == "CovidBayesianStdCensus") %>% 
  select(b_lag_temperatureMax_06, b_lag_temp_sqr,  `b_lag_temperatureMax_06:lag_precipIntensity_06`,
         b_lag_precipIntensity_06 ) %>% 
  tidyr::gather(Var, Value) 


RemoveOutliers = CheckMainParams %>% 
  group_by(Var) %>% 
  summarise(Lowest = quantile(Value, .025),
            Highest = quantile(Value, .975)) %>% 
  ungroup()

MainParamsPlt = CheckMainParams %>% 
  inner_join(RemoveOutliers, by = "Var") %>% 
  filter(Value <= Highest & Value >= Lowest) %>% 
  ggplot(., aes(x = Value, fill = "blue")) +
  geom_density() + 
  theme_classic() +
  geom_hline(yintercept = 0, colour = "white") +
  facet_wrap(~Var , scales = "free_y") +
  theme(legend.position = "bottom") +
  labs(x = "Posterior Parameter Distribution", colour = "")
  
MainParamsPlt

```

The best model combination was the interaction model, where we modeled an interaction between the max temperature on a given day, and the precipitation intensity. In Statistical Rethinking, Richard McElreath notes that plotting interactions is a much more intuitive method to understand the effect. 


```{r MarginalEffects}


CovidME = dget("WeatherData/ConditionalEffects") %>% 
  select(lag_temperatureMax_06 , lag_precipIntensity_06, estimate__, 
         lower__,  upper__, Model) %>% 
  filter(estimate__ >=0 & Model == "CovidBayesianStdCensus") %>% 
  mutate(lag_precipIntensity_06= factor(lag_precipIntensity_06, levels = c("-1", "0", "1") )) %>% 
  ggplot(., aes(x = lag_temperatureMax_06, y = estimate__)) +
  geom_line() +
  geom_ribbon(aes(ymin=lower__ , ymax=upper__), alpha=0.2) +
  theme_classic() +
  facet_wrap(~ lag_precipIntensity_06 + Model, ncol = 5) +
  labs(y = "Estimated New Cases", x= "Lagged Max Temperature")

CovidME


```

The plot above shows the interaction effect. When precipitation intensity is one standard deviation below mean, we see that as temperature increases, the estimated new cases rises. However, when precipitation intensity is one standard deviation above the mean, we see opposite. As temperature rises, estimated new cases decline. When precipitation intensity is at the mean, then temperature has little effect on estimated new cases. 

## Limitations and Conclusions

Overall, there is an effect of weather on the new cases in California. To summarize the interaction effect, good weather is associated with an increase in new cases. 

However, there are major limitations. The dataset does not have testing numbers per day. Better weather may be correlated with an increase in testing, which in turn, would lead to more new cases discovered. Testing data would significantly improve this analysis. 